# -*- coding: utf-8 -*-
"""EMNIST Balanced Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GaWxs2aDKsS1rUB6-5mu2BYWfE9IEawl
"""

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# Commented out IPython magic to ensure Python compatibility.
# %pip install keras-tuner

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

"""# Get the data:"""

import io
train = pd.read_csv('/content/gdrive/My Drive/Data/EMNIST/emnist-balanced-train.csv')

test = pd.read_csv('/content/gdrive/My Drive/Data/EMNIST/emnist-balanced-test.csv')

mapp = pd.read_csv('/content/gdrive/My Drive/Data/EMNIST/emnist-balanced-mapping.txt', delimiter=' ', index_col=0, header=None, squeeze=True)

"""# Discover and Prepare the data"""

train.head(10)

train.info()

train.head(10).T.describe()

#define a function to build up the image and rotate and flip it to get it back to its normal shape

def rotate(image):
  image = image.reshape([28, 28])
  image = np.fliplr(image)
  image = np.rot90(image)
  return image

from sklearn.model_selection import StratifiedShuffleSplit

# Create a validation set by stratified shuffle split

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

for train_index, val_index in split.split(train, train[train.columns[0]]):
  strat_train_set = train.loc[train_index]
  strat_val_set = train.loc[val_index]

X_test = test.iloc[:,1:].copy().values
y_test = test.iloc[:,0].copy().values

X_train = strat_train_set.iloc[:,1:].copy().values
y_train = strat_train_set.iloc[:,0].copy().values

X_val = strat_val_set.iloc[:,1:].copy().values
y_val = strat_val_set.iloc[:,0].copy().values

# Normalize the data

X_train = np.apply_along_axis(rotate, 1, X_train)
X_train = X_train.astype('float32') / 255

X_val = np.apply_along_axis(rotate, 1, X_val)
X_val = X_val.astype('float32') / 255

X_test = np.apply_along_axis(rotate, 1, X_test)
X_test = X_test.astype('float32') / 255



plt.figure(figsize=(10,10))
for i in range(100, 109):
  plt.subplot(330 + (i+1))
  plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))
  plt.title(chr(mapp[y_train[i]]))

plt.imshow(X_train[0], plt.get_cmap('gray'))
chr(mapp[y_train[0]])

a = train[train[train.columns[0]] == 36]

a_array = np.asarray(a.iloc[:,1:])
a_array = np.apply_along_axis(rotate, 1, a_array)
print(a_array.shape)

plt.figure(figsize=(10,10))
for i in range(100, 109):
  plt.subplot(330 + (i+1))
  plt.imshow(a_array[i], cmap=plt.get_cmap('gray'), interpolation='nearest')
  plt.axis('off')

label_indecies = np.array(mapp.values)
label_indecies.shape

for index in label_indecies:
  print(index, chr(index))

train[train.columns[0]].value_counts() / len(train)

print(len(strat_train_set), '\n')
strat_train_set[strat_train_set.columns[0]].value_counts() / len(strat_train_set)

print(len(strat_val_set), '\n')
strat_val_set[strat_val_set.columns[0]].value_counts() / len(strat_val_set)

# Reshape

X_train = X_train.reshape(-1, 28, 28, 1)
X_test = X_test.reshape(-1, 28, 28, 1)
X_val = X_val.reshape(-1, 28, 28, 1)

del train
del test

# One-Hot encoding, to be able to apply it to CNN

from keras.utils import np_utils
num_classes = 47
y_train = np_utils.to_categorical(y_train, num_classes)
y_val = np_utils.to_categorical(y_val, num_classes)
y_test = np_utils.to_categorical(y_test, num_classes)

print(X_train.shape)
print(X_val.shape)
print(y_train.shape)
print(y_val.shape)
print(y_test.shape)

# Create a model using keras-tuner

from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
def build_model(hp):
    model = Sequential()

    model.add(Conv2D(filters=hp.Int('conv1_filter', min_value=32, max_value=64, step=16)
                     , kernel_size=hp.Choice('conv1_kernel', values=[3, 5])
                     , padding = 'same', activation='relu',\
                     input_shape=(28, 28, 1)))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))
    model.add(Conv2D(filters=hp.Int('conv2_filter', min_value=48, max_value=128, step=16), kernel_size=hp.Choice('conv2_kernel', values=[3, 5]) , padding = 'same', activation='selu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Flatten())
    model.add(Dense(units=hp.Int('dense_unit', min_value=128, max_value=512, step=16), activation='relu'))
    model.add(Dropout(.3))
    model.add(Dense(units=num_classes, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer=Adam(hp.Choice('learning_rate', values=[1e-4, 1e-3])), metrics=['accuracy'])
    return model

from kerastuner import RandomSearch
from kerastuner.engine.hyperparameters import HyperParameters

tuner = RandomSearch(build_model, objective='val_accuracy', max_trials=5, directory='output1', project_name='EMNIST_Balanced_Tuned')
tuner.search(X_train, y_train, epochs=3, validation_data=(X_val, y_val))

model = tuner.get_best_models(num_models=1)[0]
# model.summary()

from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau,ModelCheckpoint

model = tuner.get_best_models(num_models=1)[0]

# Some callbacks that we're using

MCP = ModelCheckpoint('Best_points.h5',verbose=1,save_best_only=True,monitor='val_accuracy',mode='max')
ES = EarlyStopping(monitor='val_accuracy',min_delta=0,verbose=0,restore_best_weights=True,patience=3,mode='max')
RLP = ReduceLROnPlateau(monitor='val_loss',patience=3,factor=0.2,min_lr=0.0001)

# Train the model

model.fit(X_train, y_train, epochs=50, batch_size=512, verbose=1, validation_data=(X_val, y_val), initial_epoch=3, callbacks=[MCP,ES,RLP])

model.history.history

def plotgraph(epochs, acc, val_acc):
    # Plot training & validation accuracy values
    plt.plot(epochs, acc, 'b')
    plt.plot(epochs, val_acc, 'r')
    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'], loc='upper left')
    plt.show()

plt.plot(model.history.history['loss'])
plt.plot(model.history.history['val_loss'])

# Evaluate on test set

score = model.evaluate(X_test, y_test, verbose=1)
print("Test loss:", score)
print("Test accuracy:", score)

y_pred = model.predict(X_test)
y_pred

y_pred.argmax(axis=1)

y_pred = (y_pred > 0.5)

y_pred.shape

y_pred.argmax(axis=1).max()

from sklearn.metrics import confusion_matrix, classification_report
cm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))

import sys
import numpy
numpy.set_printoptions(threshold=sys.maxsize)

import seaborn as sns

plt.figure(figsize=(10, 10))
plt.matshow(cm, cmap=plt.cm.gray, fignum=1)
plt.xticks(range(0, 47))
plt.yticks(range(0, 47))

print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1)))

samples_per_class = cm.sum(axis=1, keepdims=True)
cm_normalized = cm / samples_per_class

np.fill_diagonal(cm_normalized, 0)

plt.figure(figsize=(9, 9))
plt.matshow(cm_normalized, cmap=plt.cm.gray, fignum=1)
plt.xticks(range(0, 47))
plt.yticks(range(0, 47))
plt.show()

"""Most of the errors are between classes that are so similar like:

q and 9

1 and l and I

useful youtube videos:
"""

model.save('/content/gdrive/My Drive/Data/model.h5')

# https://www.youtube.com/watch?v=u3FLVbNn9Os
# https://www.youtube.com/watch?v=am36dePheDc
# https://www.youtube.com/watch?v=ZjM_XQa5s6s
# https://www.youtube.com/watch?v=OzLAdpqm35E

